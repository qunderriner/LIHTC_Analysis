{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import altair as alt\n",
    "import cpi\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in HUD data from https://lihtc.huduser.gov/\n",
    "HUD_lihtc_filepath = 'LIHTCPUB.csv'\n",
    "hud = pd.read_csv(HUD_lihtc_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in HUD data \n",
    "syndicator_data_concat = pd.read_csv(\"syndicator_data_concat.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some quick checks on year placed in service reliability (can skip to matching sections) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I am checking for differences between year placed in service \n",
    "and construction completion date in the syndicator data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_complete= syndicator_data_concat[[\"concomdate\",\"yr_pis\",\"df\"]]\n",
    "con_complete=con_complete.dropna()\n",
    "con_complete[\"concomdate\"] = \"20\" + con_complete[\"concomdate\"].str[-2:]\n",
    "con_complete[\"diff\"] = con_complete[\"yr_pis\"].astype(int) - con_complete[\"concomdate\"].astype(int) \n",
    "con_complete[\"diff_sign\"] = np.sign(con_complete[\"diff\"])\n",
    "con_complete[\"diff_sign\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above shows that in 320 out of 3592 (8.9 percent) cases we have a construction completion date after a placed in service date. This could be plausible, if a building was started to be rented out before it was totally completed, but could also point to data errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_complete_no_zeros = con_complete[(con_complete['diff_sign'] != 0)&(con_complete['diff_sign'] != 1)]\n",
    "con_complete_no_zeros[[\"df\",\"diff_sign\"]].groupby(\"df\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vast majority of these circumstances came from a dataset where placed in service and construction completion date were given to us by the syndicator. As you can note below, the raw data from F contains both columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pd.read_csv('datasets/Syndicator F Data Set.csv',skiprows=1)#unaltered Syndicator data \n",
    "f.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a further robustness test for the PIS data, as it will be key to matching, I want to examine where it falls in relationship to the stabilizaiton date. Theoretically, the placed in service date should always procede the stabilitzation date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stab_date_check = syndicator_data_concat[[\"yr_pis\",\"stabdate\",\"df\"]].dropna()\n",
    "stab_date_check[\"stabdate\"] = \"20\" + stab_date_check[\"stabdate\"].str[-2:]\n",
    "stab_date_check[\"diff\"]= stab_date_check[\"yr_pis\"].astype(int) - stab_date_check[\"stabdate\"].astype(int)\n",
    "stab_date_check[\"diff_sign\"] = np.sign(stab_date_check[\"diff\"])\n",
    "stab_date_check[\"diff_sign\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does not do so 12.8 percent of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stab_date_check[[\"df\",\"diff_sign\"]].groupby(\"df\").describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem affects every syndicator dataset but J and K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stab_date_check_no_zeros = stab_date_check[(stab_date_check['diff_sign'] != 0)&(con_complete['diff_sign'] != 1)]\n",
    "stab_date_check_no_zeros[[\"df\",\"diff_sign\"]].groupby(\"df\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B,C,E,G,H,I were the datasets where construction completion date was substituted for PIS date. These datasets do seem to do somewhat worse on this check (especially c), but notably, F, which has the most discrepancies, is not one.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By showing this I am just trying to get a sense of how skeptical we should be of the year placed in service date to see how much wiggle room we should give matching on year. I will artfully translate this moderate skepticism into \"2 years wiggle room\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is just a quick check to see average distance between construction start date and year placed in service. Its ~1.5\n",
    "#and since we arent using months, for the later analysis rounding up to 2 seems fine\n",
    "\n",
    "syndicator_data_concat_con = syndicator_data_concat.dropna(subset=[\"yr_pis\",\"con_stdate\"])\n",
    "\n",
    "def fixdate(x):\n",
    "    if \"/\" in x:\n",
    "        year = x[-2:]\n",
    "        if year[0] == \"9\":\n",
    "            return \"19\" + year\n",
    "        else:\n",
    "            return \"20\" + year \n",
    "    else:\n",
    "        return x\n",
    "syndicator_data_concat_con[\"con_stdate\"] = syndicator_data_concat_con.con_stdate.apply(lambda x: fixdate(x))\n",
    "syndicator_data_concat_con[\"gap\"] = abs(syndicator_data_concat_con[\"yr_pis\"].astype(int) - syndicator_data_concat_con[\"con_stdate\"].astype(float))\n",
    "syndicator_data_concat_con[\"gap\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syndicator_data_concat_con[\"gap\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take subset of cols we care about\n",
    "syndicator_data_concat.rename(columns={'df':'syndicator'}, inplace=True)#rename col \n",
    "df = syndicator_data_concat[['id', 'syndicator', 'state', 'city', 'zipcode', 'yr_pis',\n",
    "                             'units_n', 'units_li', 'units_ot', 'sources_n', 'tpop',\"totalcost\",\"sources_n\",\"con_stdate\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syndicator_data_concat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.con_stdate.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df.con_stdate[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create new unique indentifiers for each syndicator \n",
    "df.rename(columns={'id':'syn_id'}, inplace=True)\n",
    "df['id'] = np.arange(df.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset HUD lihtc data for datasets we might match on plus cols we want \n",
    "hudf = hud[['hud_id', 'project', 'proj_cty', 'proj_st', 'proj_zip', 'yr_pis', 'n_units', 'li_units', \"type\",\n",
    "\"credit\",\"bond\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check on data availability of city and zipcode for syndicator data \n",
    "#we can see that we wont be able to match A & G on location. Farther down i add on G b/c it has construction type as a \n",
    "#column. A is just left out of this analysis as I don't think its possible to match on, but it also only has 93 entries.\n",
    "df.groupby('syndicator').apply(lambda x: x.notnull().mean())[['city', 'zipcode']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#match on zipcode \n",
    "zip_match = pd.merge(df.dropna(subset=['zipcode']), hudf.dropna(subset=['proj_zip']), left_on='zipcode', right_on='proj_zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do some cleaning of city name \n",
    "hudf.loc[hudf.proj_cty.notnull(), 'city'] = hudf.loc[hudf.proj_cty.notnull(), 'proj_cty'].apply(lambda x: x.lower())\n",
    "df.loc[df.city.notnull(), 'city'] = df.loc[df.city.notnull(), 'city'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#match on city \n",
    "city_match = pd.merge(df.dropna(subset=['city']), hudf.dropna(subset=['city']), left_on=['city', 'state'],\n",
    "                     right_on=['city', 'proj_st'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yr_pis match within two year for both city and zip group \n",
    "zip_match_yr = zip_match[abs(zip_match.yr_pis_x - zip_match.yr_pis_y)<=2]\n",
    "city_match_yr = city_match[abs(city_match.yr_pis_x == city_match.yr_pis_y)<=2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gap in the whole of the HUD data between the number of lihtc units and total units is about 5 percent. Which seems \n",
    "as good as a margin of error as any to use for number of units. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter by +/-5% number of units \n",
    "zip_match_yr = zip_match_yr[(abs(zip_match_yr.units_n/zip_match_yr.n_units) >= .95)&(abs(zip_match_yr.units_n/zip_match_yr.n_units) <= 1.05)] \n",
    "city_match_yr = city_match_yr[(abs(city_match_yr.units_n/city_match_yr.n_units) >= .95)&(abs(city_match_yr.units_n/city_match_yr.n_units) <= 1.05)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label type of match \n",
    "matches = zip_match_yr[~zip_match_yr.id.duplicated()]#[['id', 'hud_id']]\n",
    "matches['match_type'] = 'zip+yr'\n",
    "\n",
    "c1 = city_match_yr[~city_match_yr.id.duplicated()]#[['id', 'hud_id']]\n",
    "c1['match_type'] = 'city+yr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add both types of matches together \n",
    "matches = matches.append(c1[~(c1.id.isin(matches.id))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep year construction starts and clean it if it exists, if it does no exist us PIS - 2 \n",
    "\n",
    "#clean up construction start year\n",
    "df[\"con_stdate\"] = df[\"con_stdate\"].astype(str)\n",
    "matches[\"con_stdate_PIS\"] = matches[\"yr_pis_y\"] - 2 \n",
    "matches.loc[matches.con_stdate.notnull(), 'con_stdate'] = matches.loc[matches.con_stdate.notnull(), 'con_stdate'].apply(lambda x: fixdate(x))\n",
    "matches.loc[matches.con_stdate.isnull(), 'con_stdate'] = matches.loc[matches.con_stdate.isnull(), 'con_stdate_PIS']#.apply(lambda x: sjdfkjdkfj)\n",
    "matches = matches.rename(columns={\"con_stdate\":\"Year\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_x is syndicator data \n",
    "matches = matches.rename(columns={\"yr_pis_x\":\"yr_pis\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#want to filter for credit == 2, which means a 70% subsidy, aka 9& percent credit project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = matches[matches.credit == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are 733 nulls, which we are throwing out entirely \n",
    "matches.credit.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~30 percent of these projects are not new constufction \n",
    "matches.type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is code ot read in data from syndicator G, leaving in case we decide to do so later \n",
    "#As mentioned above, G does not have enough location data to use, but I will filter for new construction and append to our dataset\n",
    "#read in data (these are the syndicator data post Carson's initial pre-processing)\n",
    "#g = pd.read_csv(\"/Users/quinnunderriner/Desktop/Work/syndictor_lihtc/syndicator_g.csv\")\n",
    "#g = g[g.con_type == \"New Construction\"]\n",
    "#g = g.rename(columns={\"df\":\"syndicator\"})\n",
    "#append new construction G onto matches\n",
    "#matches = matches.append(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick analysis and graphing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_for_CPI(date,house_cost):\n",
    "    if date != 2020:\n",
    "        return cpi.inflate(house_cost, date)\n",
    "    else:\n",
    "        return house_cost\n",
    "def fix_year(x): #edge cases with weird year \n",
    "    if x == 8886:\n",
    "        return 1986\n",
    "    if x == 9997:\n",
    "        return 1997\n",
    "    else:\n",
    "        return x\n",
    "def clean_up_for_charts(df):\n",
    "    #make yr_pis an int, and substract two years for con_yr per discussion \n",
    "    #df[\"Year\"] = df[\"yr_pis_y\"].astype(int) - 2\n",
    "    #df = df.drop(columns={\"yr_pis\"})\n",
    "    df.Year = df.Year.apply(lambda x: fix_year(x))\n",
    "\n",
    "    df = df.dropna(subset=[\"totalcost\",\"Year\",\"units_n\"])\n",
    "    df['totalcost'] = df['totalcost'].replace({'\\$': '', ',': ''}, regex=True)\n",
    "    df['totalcost'] = df['totalcost'].astype(int)\n",
    "    #find average cost per unit, adjusted for cpi \n",
    "    \n",
    "    df[\"Year\"] = df[\"Year\"].astype(int)\n",
    "    df['totalcost_adj'] = df[[\"Year\",\"totalcost\"]].apply(lambda x: adjust_for_CPI(*x), axis=1)\n",
    "    \n",
    "    df[\"Cost Per Unit\"] = df[\"totalcost_adj\"] / df[\"units_n\"].astype(int)\n",
    "    df = df.dropna(subset=[\"Cost Per Unit\"])\n",
    "\n",
    "    #clean up name \n",
    "    df = df.rename(columns = {\"sources_n\":\"Number of Sources\",\"tpop\":\"Target Population\"})\n",
    "    df[\"Number of Sources\"] = df[\"Number of Sources\"].astype(float)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = clean_up_for_charts(matches)\n",
    "matches = matches.loc[:,~matches.columns.duplicated()] #double check no duplicate cols before putting to csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches[\"Target Population\"] = matches[\"Target Population\"].replace(\n",
    "    {\"Senior - Age Restricted\": 'Senior',\n",
    "    'Elderly':\"Senior\",\n",
    "    \"Special Needs\":\"Supportive Housing\",\n",
    "    \"Senior (62+)\":\"Senior\",\n",
    "    \"Senior (55+)\":\"Senior\",\n",
    "    \"Formerly Homeless\":\"Supportive Housing\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropped mixed \n",
    "matches[\"Target Population\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = matches.loc[:,~matches.columns.duplicated()] #double check no duplicate cols before putting to csv\n",
    "#matches.to_csv(\"big_syndicator_data_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to group by year to be able to make charts \n",
    "grouped_match = matches.groupby([\"Year\"]).mean().reset_index()\n",
    "#need to sort by year and make it a string to not have a comma (like 2,004) in the year name\n",
    "grouped_match = grouped_match.sort_values(\"Year\")\n",
    "grouped_match.Year = grouped_match.Year.astype(str)\n",
    "#make sure no duplicate cols for altair\n",
    "grouped_match = grouped_match.loc[:,~grouped_match.columns.duplicated()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(grouped_match).mark_line().encode(\n",
    "    x='Year',\n",
    "    y='Cost Per Unit').configure(background=\"#ffffff\").configure_legend().properties(\n",
    "    title={\n",
    "      \"text\": [\"Syndicator Cost Per Unit Over Time (CPI Adjusted 2020)\"]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_match = grouped_match[grouped_match.Year >= \"1991\"]\n",
    "alt.Chart(grouped_match).mark_line().encode(\n",
    "    x='Year',\n",
    "    y='Number of Sources'\n",
    ").configure(background=\"#ffffff\").configure_legend().properties(\n",
    "    title={\n",
    "      \"text\": [\"Syndicator Projects: Number of Sources Over Time\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_target_pop = matches\n",
    "matches_target_pop[\"Target Population\"] = matches_target_pop[\"Target Population\"].replace(\n",
    "    {\"Senior - Age Restricted\": 'Senior',\n",
    "    'Elderly':\"Senior\",\n",
    "    \"Special Needs\":\"Supportive Housing\",\n",
    "    \"Senior (62+)\":\"Senior\",\n",
    "    \"Senior (55+)\":\"Senior\",\n",
    "    \"Formerly Homeless\":\"Supportive Housing\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_target_pop = matches_target_pop[matches_target_pop[\"Target Population\"].isin([\"Senior\",\"Family\",\"Supportive Housing\"])]\n",
    "#need to sort by year and make it a string to not have a comma (like 2,004) in the year name\n",
    "matches_target_pop = matches_target_pop.sort_values(\"Year\")\n",
    "matches_target_pop.Year = matches_target_pop.Year.astype(str)\n",
    "matches_target_pop=matches_target_pop.groupby([\"Year\",\"Target Population\"]).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(matches_target_pop).mark_line().encode(\n",
    "    x='Year',\n",
    "    y='Cost Per Unit',\n",
    "    color=\"Target Population\"\n",
    ").configure(background=\"#ffffff\").configure_legend().properties(\n",
    "    title={\n",
    "      \"text\": [\"Syndicator Cost Per Unit Over Time by Target Population\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(matches_target_pop).mark_line().encode(\n",
    "    x='Year',\n",
    "    y='Number of Sources',\n",
    "    color=\"Target Population\"\n",
    ").configure(background=\"#ffffff\").configure_legend().properties(\n",
    "    title={\n",
    "      \"text\": [\"Syndicator Cost Per Unit Over Time by Target Population\"]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
